# 💁‍♂️ IBAS AlphaTester
* IBAS project
* Kaggle - [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
* Notion 기록 - [notion](https://fragrant-comfort-49c.notion.site/f5f92cc86a8b4a9b837d2baa8c1c98a5)
* 발표 유튜브 영상 - [youtube](https://youtu.be/D-it3wAdhEM?t=408), [ppt 자료](https://docs.google.com/presentation/d/1VokZZ61wjAl2IK0ANCC0fJ6rX8Vov257iojncti46bw/edit?usp=sharing)
* 블로그 후기 글 - [tistory](https://letsmakemyselfprogrammer.tistory.com/26?category=956261)

## ✍ process 
* EDA
* Feature Selection
* Preprocessing - [관련코드](./preprocessing.ipynb)
* Modeling - [노션참고](https://fragrant-comfort-49c.notion.site/5-Modeling-05-17-4876f9469a774e4aad4577332c32aa03)

## 👍 Preprocessing
<img src="https://github.com/Park-taenam/Kaggle_HousePrice/blob/main/image/preprocessing.png" width="25%" height="75%"/>

## 👉 Focus
#### - 통계적 검증을 통한 구현  
  - 해당 feature를 사용하는게 맞는가?
  - 해당 feature가 종속변수에 어떤 영향을 어떻게 미치는가?  
  - 독립변수들 간의 어떤 관계가 있는가?  
  - 선형회귀의 가정이 무엇이고, 왜 그런가?  
  - 그 가정을 만족했을 때의 결과와, 만족하지 않았을 때의 차이는 어떨까?  
  - 상관관계가 서로 높은 피쳐들을 제거해야만 하나? 어떤 기준으로?  
  - 다중 공선성이 높은 경우에는 어떻게 해야하는지..?  
#### - 다양한 모델링 수행
  - 선형 회귀 모델 7가지 비교
  - 어려 선형 회귀 모델의 기여도를 조절하여 overfitting 방지 
  - stacking 사용
  
  

## :smile: 아쉬웠던 점
1. 발표할 때, 결과를 정리하는 것에만 너무 신경을 썼다. 처음 듣는 사람들 입장에서는 과정이 복잡하게 느껴질 것 같았기 때문이다. 하지만 우리가 어떤식으로 작업을 했는지를 (예, 각 피쳐별로 그래프를 그려보고 분석했던 과정들) 더 보여주었으면 청중이 데이터에 대한 이해를 더 잘 했을 것 같다. 발표하고 나서 곰곰히 생각해보니까, 청중은 데이터 자체에 대한 이해가 잘 안되서 추상적으로 느껴졌을 것 같았다.

2. 피쳐 중요도를 트리 기반 모델에서 자체적으로 제공하는 것에만 의존했다. 더 공부할 시간이 부족한 게 아쉬웠다. 사실 이 부분을 알기 쉽게 파악하고 싶었는데 너무 답답했다. (선형회귀의 가정을 만족할 때와 그렇지 않을때의 결과를 비교하면서 쉽게 이해된 것과 비교된다.) 각 피쳐들이 서로 어떤 영향을 갖고, 그래서 결과에는 어떤 영향을 미치는지 시각적으로 파악하고 싶은데 아직은 어떻게 해야할지 모르겠다. 조금 찾아보니 LIME 이나 SHAP 이라고 하는게 있던데,, 더 공부해봐야겠다!!

3. 단순 호기심에, PCA로도 해보고 싶다!! PCA를 진행하면, 다중공선성 문제가 해결되는지도 궁금하다.

4. 주워 듣기로는 딥러닝 같이 계층을 깊게 만들면 다중공선성을 신경쓰지 않아도 된다고 하는데,,, 그런 기법들을 더 공부하고 싶다.

5. 다중공선선이 높다고 무조건 제거하는 것이 무조건 좋은 결과를 보장하지는 않았다. 오히려 결과를 안좋게 만들기도 했다. 결과를 예측하는데 도움이 되는 정도가, 예측에 대한 노이즈보다 더 컸기 때문이 아닐까 싶다.

6. 무서워서 하이퍼파라미터 튜닝을 제대로 하지 못했다. GridSearchCV 로 진행하는데,, 연산량이 많다보니,, 컴퓨터 본체에서 비행기 이륙하는 소리가 나서 무서웠다. 30분이 지나도록 이륙하길래 무서워서 제대로 못했다... 구글에서 제공하는 CoLab 으로 해봤는데 너무 오래걸려서 결과가 안나와서.. 실패

7. 좋은 데이터를 사용하는 것도 중요하고, 오버피팅과 언더피팅 사이의 적절한 오차수준을 유지하는 것이 중요하다는 것을 느꼈다. 이것도 꽤나 추상적이다. 적절함을 판단하는 기준이 존재할까..? 표본집단과 모집단 간의 관계를 살펴보면서, 표본집단이 모집단을 얼마나 적절하게 대표할 수 있는지도 봐야할테고,,, 그 데이터에 과적합되지도 않아야 하는,,, 그런 이상적인 경지....! 

8. 정규성을 만족하게 하는 여러가지 Transformation이 있는데, 변환들끼리의 성능을 측정할 수 있었으면 좋겠다. 이번 프로젝트에서는 log 변환보다는 boxcox 변환이 더 좋은 성능을 보였는데, 왜 그런건지 알 수가 없다. 뇌피셜로는,, 정규성를 더 잘 만족하게끔 했기 때문인걸까..? 왜도와 첨도를 측정해서 평균내보면 알 수 있겠지..?! 해봐야겠다. 이거는 함수로 만들어 놓면 유용하게 써먹을 수 있을 듯하다.

9. R square 와 같은 기초적인 통계지식에 대한 기반이 부족했다. 부전공을 해야하나... 생각이 든다. 어찌됐든 통계 이론에 대해서 더 탄탄한 지식의 필요성을 느꼈다.

10. 각 모델에 대한 이해가 부족했다. 정확히 말하면 하이퍼파라미터에 대한 이해가 부족했다. 릿지, 라쏘, 엘라스틱, SGD 등 기본적은 선형회귀는 부스트코스에서 듣고 따로 공부해서 알고 있는데,, 트리 기반 모델이나 앙상블 쪽으로 넘어가면 대충 이렇구나~ 만 알고 자세히는 알지 못한다.



